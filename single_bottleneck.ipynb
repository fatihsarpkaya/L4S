{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fabrictestbed_extensions.fablib.fablib import FablibManager as fablib_manager\n",
    "fablib = fablib_manager() \n",
    "conf = fablib.show_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define configuration for this experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_name=\"l4s-\" + fablib.get_bastion_username()\n",
    "\n",
    "node_conf = [\n",
    " {'name': \"tx0\",    'cores': 4, 'ram': 32, 'disk': 20, 'image': 'default_ubuntu_22', 'packages': ['iperf3', 'net-tools', 'moreutils']}, \n",
    " {'name': \"tx1\",    'cores': 4, 'ram': 32, 'disk': 20, 'image': 'default_ubuntu_22', 'packages': ['iperf3', 'net-tools', 'moreutils']}, \n",
    " {'name': \"router\", 'cores': 4, 'ram': 32, 'disk': 20, 'image': 'default_ubuntu_22', 'packages': ['iperf3', 'net-tools', 'moreutils']}, \n",
    " {'name': \"delay\", 'cores': 4, 'ram': 32, 'disk': 20, 'image': 'default_ubuntu_22', 'packages': ['iperf3', 'net-tools', 'moreutils']}, \n",
    " {'name': \"rx0\",    'cores': 4, 'ram': 32, 'disk': 20, 'image': 'default_ubuntu_22', 'packages': ['iperf3', 'net-tools', 'moreutils']}, \n",
    " {'name': \"rx1\",    'cores': 4, 'ram': 32, 'disk': 20, 'image': 'default_ubuntu_22', 'packages': ['iperf3', 'net-tools', 'moreutils']}\n",
    "]\n",
    "net_conf = [\n",
    " {\"name\": \"net-tx\", \"subnet\": \"10.0.0.0/24\", \"nodes\": [{\"name\": \"tx0\",   \"addr\": \"10.0.0.100\"}, {\"name\": \"tx1\",   \"addr\": \"10.0.0.101\"}, {\"name\": \"delay\", \"addr\": \"10.0.0.2\"}]},\n",
    " {\"name\": \"net-delay-router\", \"subnet\": \"10.0.2.0/24\", \"nodes\": [{\"name\": \"delay\",   \"addr\": \"10.0.2.2\"}, {\"name\": \"router\", \"addr\": \"10.0.2.1\"}]},\n",
    " {\"name\": \"net-rx\", \"subnet\": \"10.0.5.0/24\", \"nodes\": [{\"name\": \"router\",   \"addr\": \"10.0.5.1\"}, {\"name\": \"rx0\", \"addr\": \"10.0.5.100\"}, {\"name\": \"rx1\", \"addr\": \"10.0.5.101\"}]}\n",
    "\n",
    "]\n",
    "route_conf = [\n",
    " {\"addr\": \"10.0.5.0/24\", \"gw\": \"10.0.0.2\", \"nodes\": [\"tx0\", \"tx1\"]}, \n",
    " {\"addr\": \"10.0.5.0/24\", \"gw\": \"10.0.2.1\", \"nodes\": [\"delay\"]},\n",
    "\n",
    " {\"addr\": \"10.0.0.0/24\", \"gw\": \"10.0.5.1\", \"nodes\": [\"rx0\", \"rx1\"]},\n",
    " {\"addr\": \"10.0.0.0/24\", \"gw\": \"10.0.2.2\", \"nodes\": [\"router\"]}\n",
    "\n",
    "]\n",
    "exp_conf = {'cores': sum([ n['cores'] for n in node_conf]), 'nic': sum([len(n['nodes']) for n in net_conf]) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reserve resources\n",
    "\n",
    "Now, we are ready to reserve resources!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, make sure you don’t already have a slice with this name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    slice = fablib.get_slice(slice_name)\n",
    "    print(\"You already have a slice by this name!\")\n",
    "    print(\"If you previously reserved resources, skip to the 'log in to resources' section.\")\n",
    "except:\n",
    "    print(\"You don't have a slice named %s yet.\" % slice_name)\n",
    "    print(\"Continue to the next step to make one.\")\n",
    "    slice = fablib.new_slice(name=slice_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will select a random site that has sufficient resources for our experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    site_name = fablib.get_random_site()\n",
    "    if ( (fablib.resources.get_core_available(site_name) > 1.2*exp_conf['cores']) and\n",
    "        (fablib.resources.get_component_available(site_name, 'SharedNIC-ConnectX-6') > 1.2**exp_conf['nic']) ):\n",
    "        break\n",
    "\n",
    "fablib.show_site(site_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will add hosts and network segments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell sets up the nodes\n",
    "for n in node_conf:\n",
    "    slice.add_node(name=n['name'], site=site_name, \n",
    "                   cores=n['cores'], \n",
    "                   ram=n['ram'], \n",
    "                   disk=n['disk'], \n",
    "                   image=n['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell sets up the network segments\n",
    "for n in net_conf:\n",
    "    ifaces = [slice.get_node(node[\"name\"]).add_component(model=\"NIC_Basic\", \n",
    "                                                 name=n[\"name\"]).get_interfaces()[0] for node in n['nodes'] ]\n",
    "    slice.add_l2network(name=n[\"name\"], type='L2Bridge', interfaces=ifaces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell submits our request to the FABRIC site. The output of this cell will update automatically as the status of our request changes.\n",
    "\n",
    "-   While it is being prepared, the “State” of the slice will appear as “Configuring”.\n",
    "-   When it is ready, the “State” of the slice will change to “StableOK”.\n",
    "\n",
    "You may prefer to walk away and come back in a few minutes (for simple slices) or a few tens of minutes (for more complicated slices with many resources)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice.get_state()\n",
    "slice.wait_ssh(progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extend your slice\n",
    "\n",
    "If you don’t plan to finish an experiment in one day, you can extend your slice. The following cell extends your reservation for 7 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from datetime import timezone\n",
    "from datetime import timedelta\n",
    "\n",
    "# Set end date to 7 days from now\n",
    "end_date = (datetime.now(timezone.utc) + timedelta(days=7)).strftime(\"%Y-%m-%d %H:%M:%S %z\")\n",
    "slice.renew(end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in slice.get_nodes():\n",
    "    # Download and unzip the kernel package\n",
    "    node.execute(\"wget https://github.com/L4STeam/linux/releases/download/testing-build/l4s-testing.zip\")\n",
    "    node.execute(\"sudo apt install unzip\")\n",
    "    node.execute(\"unzip l4s-testing.zip\")\n",
    "    \n",
    "    # Install the kernel packages and update GRUB\n",
    "    node.execute(\"sudo dpkg --install debian_build/*\")\n",
    "    node.execute(\"sudo update-grub\")\n",
    "    node.execute(\"sudo reboot\")\n",
    "\n",
    "# wait for all nodes to come back up\n",
    "slice.wait_ssh(progress=True)\n",
    "for node in slice.get_nodes():\n",
    "    # check kernel version\n",
    "    node.execute(\"hostname; uname -a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inital configuration for the senders\n",
    "slice.get_node(name=\"tx0\").execute(\"sudo modprobe tcp_prague\")\n",
    "slice.get_node(name=\"tx0\").execute(\"sudo sysctl -w net.ipv4.tcp_congestion_control=prague\")\n",
    "slice.get_node(name=\"tx0\").execute(\"sudo sysctl -w net.ipv4.tcp_ecn=3\")\n",
    "\n",
    "slice.get_node(name=\"tx1\").execute(\"sudo sysctl -w net.ipv4.tcp_congestion_control=cubic\")\n",
    "slice.get_node(name=\"tx1\").execute(\"sudo sysctl -w net.ipv4.tcp_ecn=1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configuration for DUALPI2 bottleneck\n",
    "cmd_dualpi2=\"\"\"sudo apt-get update\n",
    "sudo apt -y install git gcc make bison flex libdb-dev libelf-dev pkg-config libbpf-dev libmnl-dev libcap-dev libatm1-dev selinux-utils libselinux1-dev\n",
    "sudo git clone https://github.com/L4STeam/iproute2.git && cd iproute2\n",
    "sudo ./configure\n",
    "sudo make\n",
    "sudo make install\"\"\"\n",
    "slice.get_node(name=\"router\").execute(cmd_dualpi2)\n",
    "slice.get_node(name=\"router\").execute(\"sudo modprobe sch_dualpi2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure resources\n",
    "\n",
    "Next, we will configure the resources so they are ready to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice = fablib.get_slice(name=slice_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install packages\n",
    "# this will take a while and will run in background while you do other steps\n",
    "for n in node_conf:\n",
    "    if len(n['packages']):\n",
    "        node = slice.get_node(n['name'])\n",
    "        pkg = \" \".join(n['packages'])\n",
    "        node.execute_thread(\"sudo apt update; sudo apt -y install %s\" % pkg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bring interfaces up and either assign an address (if there is one) or flush address\n",
    "from ipaddress import ip_address, IPv4Address, IPv4Network\n",
    "\n",
    "for net in net_conf:\n",
    "    for n in net['nodes']:\n",
    "        if_name = n['name'] + '-' + net['name'] + '-p1'\n",
    "        iface = slice.get_interface(if_name)\n",
    "        iface.ip_link_up()\n",
    "        if n['addr']:\n",
    "            iface.ip_addr_add(addr=n['addr'], subnet=IPv4Network(net['subnet']))\n",
    "        else:\n",
    "            iface.get_node().execute(\"sudo ip addr flush dev %s\"  % iface.get_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare a \"hosts\" file that has names and addresses of every node\n",
    "hosts_txt = [ \"%s\\t%s\" % ( n['addr'], n['name'] ) for net in net_conf  for n in net['nodes'] if type(n) is dict and n['addr']]\n",
    "for n in slice.get_nodes():\n",
    "    for h in hosts_txt:\n",
    "        n.execute(\"echo %s | sudo tee -a /etc/hosts\" % h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable IPv4 forwarding on all nodes\n",
    "for n in slice.get_nodes():\n",
    "    n.execute(\"sudo sysctl -w net.ipv4.ip_forward=1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up static routes\n",
    "for rt in route_conf:\n",
    "    for n in rt['nodes']:\n",
    "        slice.get_node(name=n).ip_route_add(subnet=IPv4Network(rt['addr']), gateway=rt['gw'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn off segmentation offload on interfaces\n",
    "for iface in slice.get_interfaces():\n",
    "    iface_name = iface.get_device_name()\n",
    "    n = iface.get_node()\n",
    "    offloads = [\"gro\", \"lro\", \"gso\", \"tso\"]\n",
    "    for offload in offloads:\n",
    "        n.execute(\"sudo ethtool -K %s %s off\" % (iface_name, offload))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw the network topology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will draw the network topology, for your reference. The interface name and addresses of each experiment interface will be shown on the drawing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_nets = [(n.get_name(), {'color': 'lavender'}) for n in slice.get_l2networks() ]\n",
    "l3_nets = [(n.get_name(), {'color': 'pink'}) for n in slice.get_l3networks() ]\n",
    "hosts   =   [(n.get_name(), {'color': 'lightblue'}) for n in slice.get_nodes()]\n",
    "nodes = l2_nets + l3_nets + hosts\n",
    "ifaces = [iface.toDict() for iface in slice.get_interfaces()]\n",
    "edges = [(iface['network'], iface['node'], \n",
    "          {'label': iface['physical_dev'] + '\\n' + iface['ip_addr'] + '\\n' + iface['mac']}) for iface in ifaces]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(len(nodes),len(nodes)))\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(nodes)\n",
    "G.add_edges_from(edges)\n",
    "pos = nx.spring_layout(G)\n",
    "nx.draw(G, pos, node_shape='s',  \n",
    "        node_color=[n[1]['color'] for n in nodes], \n",
    "        node_size=[len(n[0])*400 for n in nodes],  \n",
    "        with_labels=True);\n",
    "nx.draw_networkx_edge_labels(G,pos,\n",
    "                             edge_labels=nx.get_edge_attributes(G,'label'),\n",
    "                             font_color='gray',  font_size=8, rotate=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log into resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are finally ready to log in to our resources over SSH! Run the following cells, and observe the table output - you will see an SSH command for each of the resources in your topology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "slice_info = [{'Name': n.get_name(), 'SSH command': n.get_ssh_command()} for n in slice.get_nodes()]\n",
    "pd.DataFrame(slice_info).set_index('Name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can open an SSH session on any of the resources as follows:\n",
    "\n",
    "-   in Jupyter, from the menu bar, use File \\> New \\> Terminal to open a new terminal.\n",
    "-   copy an SSH command from the table, and paste it into the terminal. (Note that each SSH command is a single line, even if the display wraps the text to a second line! When you copy and paste it, paste it all together.)\n",
    "\n",
    "You can repeat this process (open several terminals) to start a session on each resource. Each terminal session will have a tab in the Jupyter environment, so that you can easily switch between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nodes and instances\n",
    "\n",
    "tx0_node = slice.get_node(name=\"tx0\")\n",
    "tx1_node = slice.get_node(name=\"tx1\")\n",
    "rx0_node = slice.get_node(name=\"rx0\")\n",
    "rx1_node = slice.get_node(name=\"rx1\")\n",
    "delay_node = slice.get_node(name=\"delay\")\n",
    "router_node = slice.get_node(name=\"router\")\n",
    "# interfaces\n",
    "\n",
    "tx0_egress_iface  = tx0_node.get_interface(network_name = \"net-tx\")\n",
    "tx1_egress_iface  = tx1_node.get_interface(network_name = \"net-tx\")\n",
    "\n",
    "delay_ingress_tx_iface  = delay_node.get_interface(network_name = \"net-tx\")\n",
    "delay_egress_iface  = delay_node.get_interface(network_name = \"net-delay-router\")\n",
    "delay_ingress_tx_name = delay_ingress_tx_iface.get_device_name()\n",
    "delay_egress_name = delay_egress_iface.get_device_name()\n",
    "\n",
    "router_ingress_iface  = router_node.get_interface(network_name = \"net-delay-router\")\n",
    "router_egress_iface  = router_node.get_interface(network_name = \"net-rx\")\n",
    "router_egress_name  = router_egress_iface.get_device_name()\n",
    "\n",
    "\n",
    "rx0_ingress_iface  = rx0_node.get_interface(network_name = \"net-rx\")\n",
    "rx1_ingress_iface  = rx1_node.get_interface(network_name = \"net-rx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate full factorial experiment\n",
    "import itertools\n",
    "\n",
    "exp_factors = {\n",
    "    'n_bdp': [0.5, 2, 5, 10],  # n x bandwidth delay product\n",
    "    'btl_capacity': [100, 1000],\n",
    "    'base_rtt': [5, 10, 50, 100],\n",
    "    'aqm': ['FIFO', 'single_queue_FQ', 'Codel', 'FQ', 'FQ_Codel', 'DualPI2'],\n",
    "    #'aqm': ['single_queue_FQ', 'Codel', 'FQ', 'FQ_Codel', 'DualPI2'],\n",
    "    'ecn_threshold': [1, 5, 20],\n",
    "    'ecn_fallback': [0, 1],  #fallback algorithm, it falls back when it detects single queue classic ECN bottleneck # 0: OFF, 1: ON\n",
    "    'rx0_ecn': [0, 1, 2],  # 0: noecn, 1: ecn, 2: accecn\n",
    "    'rx1_ecn': [0, 1],  # 0: noecn, 1: ecn\n",
    "    'cc_tx0': [\"prague\"],\n",
    "    'cc_tx1': [\"cubic\"],\n",
    "    'trial': [1, 2, 3, 4, 5]\n",
    "}\n",
    "\n",
    "factor_names = [k for k in exp_factors]\n",
    "factor_lists = list(itertools.product(*exp_factors.values()))\n",
    "\n",
    "exp_lists = []\n",
    "\n",
    "seen_combinations = set()\n",
    "\n",
    "for factor_l in factor_lists:\n",
    "    temp_dict = dict(zip(factor_names, factor_l))\n",
    "    if temp_dict['n_bdp'] * temp_dict['base_rtt'] >= temp_dict['ecn_threshold']:\n",
    "        if temp_dict['aqm'] == 'FIFO':\n",
    "            del temp_dict['ecn_threshold']\n",
    "        # Convert dict to a frozenset for set operations\n",
    "        fs = frozenset(temp_dict.items())\n",
    "    \n",
    "        if fs not in seen_combinations:\n",
    "            seen_combinations.add(fs)\n",
    "            exp_lists.append(temp_dict)\n",
    "\n",
    "data_dir_tx0 = slice_name + 'singlebottleneck'+\"-tx0\"\n",
    "data_dir_tx1 = slice_name + 'singlebottleneck'+\"-tx1\"\n",
    "\n",
    "print(\"Number of experiments:\",len(exp_lists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run experiments\n",
    "import time\n",
    "d = 60 #duration in seconds\n",
    "\n",
    "em = [delay_ingress_tx_name, delay_egress_name]\n",
    "\n",
    "commands_noecn='''\n",
    "sudo sysctl -w net.ipv4.tcp_congestion_control=cubic  \n",
    "sudo sysctl -w net.ipv4.tcp_ecn=0''' #tcp_prague, no ECN\n",
    "\n",
    "commands_ecn='''\n",
    "sudo sysctl -w net.ipv4.tcp_congestion_control=cubic  \n",
    "sudo sysctl -w net.ipv4.tcp_ecn=1''' #tcp_prague, ECN\n",
    "\n",
    "commands_accecn='''\n",
    "sudo sysctl -w net.ipv4.tcp_congestion_control=prague  \n",
    "sudo sysctl -w net.ipv4.tcp_ecn=3''' #tcp_prague, AccECN\n",
    "\n",
    "server_ecn_list=[commands_noecn, commands_ecn, commands_accecn]\n",
    "\n",
    "for exp in exp_lists:\n",
    "\n",
    "    # check if we already ran this experiment\n",
    "    # (allow stop/resume)\n",
    "    name_tx0=\"%s_%0.1f_%d_%d_%s_%s_%d_%d_%d_%d\" % (exp['cc_tx0'],exp['n_bdp'], exp['btl_capacity'], exp['base_rtt'], exp['aqm'], str(exp.get('ecn_threshold', 'none')), exp['ecn_fallback'], exp['rx0_ecn'], exp['rx1_ecn'], exp['trial'])\n",
    "    name_tx1=\"%s_%0.1f_%d_%d_%s_%s_%d_%d_%d_%d\" % (exp['cc_tx1'],exp['n_bdp'], exp['btl_capacity'], exp['base_rtt'], exp['aqm'], str(exp.get('ecn_threshold', 'none')), exp['ecn_fallback'], exp['rx0_ecn'], exp['rx1_ecn'], exp['trial'])\n",
    "    \n",
    "    file_out_tx0_json = name_tx0+\"-result.json\"\n",
    "    file_out_tx0_ss = name_tx0+\"-ss.txt\"\n",
    "    stdout_tx0_json, stderr_tx0_json = tx0_node.execute(\"ls \" + file_out_tx0_json, quiet=True) \n",
    "    stdout_tx0_ss, stderr_tx0_ss = tx0_node.execute(\"ls \" + file_out_tx0_ss, quiet=True)\n",
    "    \n",
    "    file_out_tx1_json =name_tx1+\"-result.json\"\n",
    "    file_out_tx1_ss = name_tx1+\"-ss.txt\"\n",
    "    stdout_tx1_json, stderr_tx1_json = tx1_node.execute(\"ls \" + file_out_tx1_json, quiet=True) \n",
    "    stdout_tx1_ss, stderr_tx1_ss = tx1_node.execute(\"ls \" + file_out_tx1_ss, quiet=True) \n",
    "\n",
    "    if len(stdout_tx0_json) and len(stdout_tx0_ss) and len(stdout_tx1_json) and len(stdout_tx1_ss):\n",
    "        print(\"Already have \" + name_tx0 + \" and \"+ name_tx1 + \", skipping\")\n",
    "\n",
    "    elif len(stderr_tx0_json) or len(stderr_tx0_ss) or len(stderr_tx1_json) or len(stderr_tx1_ss):\n",
    "        print(\"Running experiment to generate \" + name_tx0 + \" and \"+ name_tx1) \n",
    "        \n",
    "        # delay at emulator\n",
    "        for e in em:\n",
    "            cmds = \"sudo tc qdisc replace dev {iface} root netem delay {owd}ms limit 60000\".format(iface=e, owd=exp['base_rtt']/2)\n",
    "            delay_node.execute(cmds)\n",
    "        \n",
    "        # fixed values\n",
    "        btl_limit    = int(1000*exp['n_bdp']*exp['btl_capacity']*exp['base_rtt']/8) # limit of the bottleneck, n_bdp x BDP in bytes \n",
    "        packet_number=int(btl_limit/1500)+1\n",
    "        \n",
    "        \n",
    "        #ecn-fallback configuration\n",
    "               \n",
    "        commands = \"cd /sys/module/tcp_prague/parameters && echo {value} | sudo tee prague_ecn_fallback\".format(value=str(exp['ecn_fallback']))\n",
    "        tx0_node.execute(commands)\n",
    "        \n",
    "        #receiver ecn configuration\n",
    "        rx0_node.execute(server_ecn_list[exp['rx0_ecn']])\n",
    "        rx1_node.execute(server_ecn_list[exp['rx1_ecn']])\n",
    "        \n",
    "        #aqm type selection\n",
    "        cmds_prefix = '''\n",
    "            sudo tc qdisc del dev {iface} root\n",
    "            sudo tc qdisc replace dev {iface} root handle 1: htb default 3 \n",
    "            sudo tc class add dev {iface} parent 1: classid 1:3 htb rate {capacity}mbit \n",
    "            '''.format(iface=router_egress_name, capacity=exp['btl_capacity'], buffer=btl_limit)\n",
    "        cmds_specific = {\n",
    "        'FIFO': \"sudo tc qdisc add dev {iface} parent 1:3 handle 3: bfifo limit {buffer}\".format(iface=router_egress_name, buffer=btl_limit),\n",
    "        'single_queue_FQ': \"sudo tc qdisc replace dev {iface} parent 1:3 handle 3: fq limit {packet_limit} flow_limit {packet_limit} orphan_mask 0 ce_threshold {threshold}ms\".format(iface=router_egress_name, packet_limit=packet_number, threshold=exp.get('ecn_threshold', 0)),\n",
    "        'Codel': \"sudo tc qdisc replace dev {iface} parent 1:3 handle 3: codel limit {packet_limit} target {target}ms interval 100ms ecn ce_threshold {threshold}ms\".format(iface=router_egress_name, packet_limit=packet_number, target=exp['base_rtt']*exp['n_bdp'], threshold=exp.get('ecn_threshold', 0)),\n",
    "        'FQ': \"sudo tc qdisc replace dev {iface} parent 1:3 handle 3: fq limit {packet_limit} flow_limit {packet_limit} ce_threshold {threshold}ms\".format(iface=router_egress_name, packet_limit=packet_number, threshold=exp.get('ecn_threshold', 0)),\n",
    "        'FQ_Codel': \"sudo tc qdisc replace dev {iface} parent 1:3 handle 3: fq_codel limit {packet_limit} target {target}ms interval 100ms ecn ce_threshold {threshold}ms\".format(iface=router_egress_name, packet_limit=packet_number, target=exp['base_rtt']*exp['n_bdp'], threshold=exp.get('ecn_threshold', 0)),\n",
    "        'DualPI2': \"sudo tc qdisc add dev {iface} parent 1:3 handle 3: dualpi2 target {threshold}ms\".format(iface=router_egress_name, threshold=exp.get('ecn_threshold', 0))\n",
    "        }\n",
    "\n",
    "        router_node.execute(cmds_prefix)\n",
    "        router_node.execute(cmds_specific[ exp['aqm'] ])\n",
    "            \n",
    "        rx0_node.execute(\"killall iperf3\")\n",
    "        rx1_node.execute(\"killall iperf3\")\n",
    "        \n",
    "        ss_tx0_script=\"rm -f {flow}-ss.txt; start_time=$(date +%s); while true; do ss --no-header -eipn dst 10.0.5.100 | ts '%.s' | tee -a {flow}-ss.txt; current_time=$(date +%s); elapsed_time=$((current_time - start_time));  if [ $elapsed_time -ge {duration} ]; then break; fi; sleep 0.1; done;\"\n",
    "        ss_tx1_script=\"rm -f {flow}-ss.txt; start_time=$(date +%s); while true; do ss --no-header -eipn dst 10.0.5.101 | ts '%.s' | tee -a {flow}-ss.txt; current_time=$(date +%s); elapsed_time=$((current_time - start_time));  if [ $elapsed_time -ge {duration} ]; then break; fi; sleep 0.1; done;\"\n",
    "\n",
    "        #print(\"Starting experiment with {1} bdp {2} capacity {3} rtt {4} {5} thrshold {6} ecn_fallback {7} rx0 {8} rx1 for {duration} seconds\".format(duration=d, 1=exp['n_bdp'], 2=exp['btl_capacity'], 3=exp['base_rtt'], 4=exp['aqm'], 5=exp['ecn_threshold'], 6= exp['ecn_fallback'], 7=exp['rx0_ecn'], 8=exp['rx1_ecn']))\n",
    "        \n",
    "        rx0_node.execute(\"iperf3 -s -1 -p 4000 -D\")\n",
    "        rx1_node.execute(\"iperf3 -s -1 -p 5000 -D\")\n",
    "        \n",
    "        tx0_node.execute_thread(ss_tx0_script.format(flow=name_tx0, duration=d))\n",
    "        tx1_node.execute_thread(ss_tx1_script.format(flow=name_tx1, duration=d))\n",
    "        \n",
    "        tx0_node.execute_thread(\"sleep 1; iperf3 -c 10.0.5.100 -t {duration} -P {flows} -C {cc} -p 4000 -J > {flow}-result.json\".format(flow =name_tx0, duration=d, flows=1, cc=exp['cc_tx0']))\n",
    "        stdout, stderr = tx1_node.execute(\"sleep 1; iperf3 -c 10.0.5.101 -t {duration} -P {flows} -C {cc} -p 5000 -J > {flow}-result.json\".format(flow =name_tx1, duration=d, flows=1, cc=exp['cc_tx1']))\n",
    "        time.sleep(3)  # time.sleep(1)\n",
    "        \n",
    "print(\"finished\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for exp in exp_lists:\n",
    "    name_tx0=\"%s_%0.1f_%d_%d_%s_%s_%d_%d_%d\" % (exp['cc_tx0'],exp['n_bdp'], exp['btl_capacity'], exp['base_rtt'], exp['aqm'], str(exp.get('ecn_threshold', 'none')), exp['ecn_fallback'], exp['rx0_ecn'], exp['rx1_ecn'])\n",
    "    name_tx1=\"%s_%0.1f_%d_%d_%s_%s_%d_%d_%d\" % (exp['cc_tx1'],exp['n_bdp'], exp['btl_capacity'], exp['base_rtt'], exp['aqm'], str(exp.get('ecn_threshold', 'none')), exp['ecn_fallback'], exp['rx0_ecn'], exp['rx1_ecn'])\n",
    "    \n",
    "    \n",
    "    file_out_tx0_csv = name_tx0+\"-ss.csv\"\n",
    "    stdout_tx0_csv, stderr_tx0_csv = tx0_node.execute(\"ls \" + file_out_tx0_csv, quiet=True) \n",
    "    \n",
    "    file_out_tx1_csv = name_tx1+\"-ss.csv\"\n",
    "    stdout_tx1_csv, stderr_tx1_csv = tx1_node.execute(\"ls \" + file_out_tx1_csv, quiet=True) \n",
    "\n",
    "    if len(stdout_tx0_csv) and len(stdout_tx1_csv):\n",
    "        print(\"Already have \" + name_tx0 + \" and \"+ name_tx1 + \", skipping\")\n",
    "\n",
    "    elif len(stderr_tx0_csv) or len(stderr_tx1_csv):\n",
    "        print(\"Running to generate csv files \" + name_tx0 + \" and \"+ name_tx1)\n",
    "        \n",
    "        \n",
    "    #tx0_node.download_file(\"/home/fabric/work/{flow}-result.json\".format(flow=name_prague),\"/home/ubuntu/{f_type}-result.json\".format(f_type=name_prague))\n",
    "    #tx1_node.download_file(\"/home/fabric/work/{flow}-result.json\".format(flow=name_cubic),\"/home/ubuntu/{f_type}-result.json\".format(f_type=name_cubic))\n",
    "    #tx0_node.download_file(\"/home/fabric/work/{flow}-ss.txt\".format(flow=name_prague),\"/home/ubuntu/{f_type}-ss.txt\".format(f_type=name_prague))\n",
    "    #tx1_node.download_file(\"/home/fabric/work/{flow}-ss.txt\".format(flow=name_cubic),\"/home/ubuntu/{f_type}-ss.txt\".format(f_type=name_cubic))\n",
    "\n",
    "\n",
    "    \n",
    "        ss_tx0_script_processing=\"\"\"\n",
    "\n",
    "        f_1={types}; \n",
    "        rm -f ${{f_1}}-ss.csv;\n",
    "        cat ${{f_1}}-ss.txt | sed -e \":a; /<->$/ {{ N; s/<->\\\\n//; ba; }}\"  | grep \"iperf3\" | grep -v \"SYN-SENT\"> ${{f_1}}-ss-processed.txt; \n",
    "        cat ${{f_1}}-ss-processed.txt | awk '{{print $1}}' > ts-${{f_1}}.txt; \n",
    "        cat ${{f_1}}-ss-processed.txt | grep -oP '\\\\bcwnd:.*?(\\s|$)' | awk -F '[:,]' '{{print $2}}' | tr -d ' ' > cwnd-${{f_1}}.txt; \n",
    "        cat ${{f_1}}-ss-processed.txt | grep -oP '\\\\brtt:.*?(\\s|$)' | awk -F '[:,]' '{{print $2}}' | tr -d ' '  | cut -d '/' -f 1   > srtt-${{f_1}}.txt; \n",
    "        cat ${{f_1}}-ss-processed.txt | grep -oP '\\\\bfd=.*?(\\s|$)' | awk -F '[=,]' '{{print $2}}' | tr -d ')' | tr -d ' '   > fd-${{f_1}}.txt;\n",
    "        paste ts-${{f_1}}.txt fd-${{f_1}}.txt cwnd-${{f_1}}.txt srtt-${{f_1}}.txt -d ',' > ${{f_1}}-ss.csv;\"\"\".format(types=name_tx0)\n",
    "     \n",
    "    \n",
    "        tx0_node.execute(ss_tx0_script_processing)\n",
    "\n",
    "\n",
    "        ss_tx1_script_processing=\"\"\"\n",
    "\n",
    "        f_2={types};\n",
    "        rm -f ${{f_2}}-ss.csv;\n",
    "        cat ${{f_2}}-ss.txt | sed -e \":a; /<->$/ {{ N; s/<->\\\\n//; ba; }}\"  | grep \"iperf3\" | grep -v \"SYN-SENT\" > ${{f_2}}-ss-processed.txt; \n",
    "        cat ${{f_2}}-ss-processed.txt | awk '{{print $1}}' > ts-${{f_2}}.txt; \n",
    "        cat ${{f_2}}-ss-processed.txt | grep -oP '\\\\bcwnd:.*?(\\s|$)' |  awk -F '[:,]' '{{print $2}}' | tr -d ' ' > cwnd-${{f_2}}.txt; \n",
    "        cat ${{f_2}}-ss-processed.txt | grep -oP '\\\\brtt:.*?(\\s|$)' |  awk -F '[:,]' '{{print $2}}' | tr -d ' '  | cut -d '/' -f 1   > srtt-${{f_2}}.txt; \n",
    "        cat ${{f_2}}-ss-processed.txt | grep -oP '\\\\bfd=.*?(\\s|$)' |  awk -F '[=,]' '{{print $2}}' | tr -d ')' | tr -d ' '   > fd-${{f_2}}.txt;\n",
    "        paste ts-${{f_2}}.txt fd-${{f_2}}.txt cwnd-${{f_2}}.txt srtt-${{f_2}}.txt -d ',' > ${{f_2}}-ss.csv;\"\"\".format(types=name_tx1)\n",
    "\n",
    "\n",
    "        tx1_node.execute(ss_tx1_script_processing)\n",
    "\n",
    "    #tx0_node.download_file(\"/home/fabric/work/{f_type}-ss.csv\".format(f_type=name_prague),\"/home/ubuntu/{f_type}-ss.csv\".format(f_type=name_prague))\n",
    "    #tx1_node.download_file(\"/home/fabric/work/{f_type}-ss.csv\".format(f_type=name_cubic),\"/home/ubuntu/{f_type}-ss.csv\".format(f_type=name_cubic))\n",
    "\n",
    "\n",
    "\n",
    "#tx0_node.execute('rm -r '+data_dir_tx0)\n",
    "tx0_node.execute('mkdir '+data_dir_tx0)\n",
    "\n",
    "tx0_node.execute('mv *.json '+ data_dir_tx0)\n",
    "tx0_node.execute('mv *.txt '+ data_dir_tx0)\n",
    "tx0_node.execute('mv *.csv '+ data_dir_tx0)\n",
    "\n",
    "tx0_node.execute('tar -czvf '+data_dir_tx0+ '.tgz ' +  data_dir_tx0)\n",
    "#tx0_node.download_file(data_dir_tx0+'.tgz ', '/home/ubuntu/' + data_dir_tx0+ '.tgz')\n",
    "\n",
    "\n",
    "#tx1_node.execute('rm -r '+data_dir_tx1)\n",
    "tx1_node.execute('mkdir '+data_dir_tx1)\n",
    "\n",
    "tx1_node.execute('mv *.json '+ data_dir_tx1)\n",
    "tx1_node.execute('mv *.txt '+ data_dir_tx1)\n",
    "tx1_node.execute('mv *.csv '+ data_dir_tx1)\n",
    "        \n",
    "tx1_node.execute('tar -czvf '+data_dir_tx1+ '.tgz ' +  data_dir_tx1)\n",
    "#tx1_node.download_file(data_dir_tx1+'.tgz ', '/home/ubuntu/' + data_dir_tx1+ '.tgz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, tarfile\n",
    "\n",
    "# remove previously stored data files\n",
    "#shutil.rmtree(data_dir_tx0)\n",
    "#shutil.rmtree(data_dir_tx1)\n",
    "\n",
    "# extract tar files \n",
    "with tarfile.open(data_dir_tx0+'.tgz ', 'r:gz') as tar:\n",
    "    tar.extractall()\n",
    "    \n",
    "with tarfile.open(data_dir_tx1+'.tgz ', 'r:gz') as tar:\n",
    "    tar.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can download the .tgz file from the node to the local machine and use plot_on_local_barplot.py file to see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "\n",
    "from statistics import mean\n",
    "\n",
    "\n",
    "#get all throughput and SRTT data\n",
    "\n",
    "throughput_data = {}  # Initialize the dictionary\n",
    "\n",
    "srtt_data = {}\n",
    "\n",
    "for exp in exp_lists:\n",
    "\n",
    "    name_tx0=\"%s_%0.1f_%d_%d_%s_%s_%d_%d_%d\" % (exp['cc_tx0'],exp['n_bdp'], exp['btl_capacity'], exp['base_rtt'], exp['aqm'], str(exp.get('ecn_threshold', 'none')), exp['ecn_fallback'], exp['rx0_ecn'], exp['rx1_ecn'])\n",
    "    name_tx1=\"%s_%0.1f_%d_%d_%s_%s_%d_%d_%d\" % (exp['cc_tx1'],exp['n_bdp'], exp['btl_capacity'], exp['base_rtt'], exp['aqm'], str(exp.get('ecn_threshold', 'none')), exp['ecn_fallback'], exp['rx0_ecn'], exp['rx1_ecn'])\n",
    "    \n",
    "\n",
    "    # Load the JSON output file into a Python object\n",
    "    \n",
    "    ##### Average Throughput for Each Flow ****\n",
    "    \n",
    "    with open(\"/home/fabric/work/\"+data_dir_tx0+\"/{flow1}-result.json\".format(flow1=name_tx0), \"r\") as f:\n",
    "        iperf3_data = json.load(f)\n",
    "\n",
    "    throughput_data[name_tx0]  = iperf3_data['end']['sum_received']['bits_per_second']/(1000000*1) # to convert Mbit\n",
    "\n",
    "\n",
    "    with open(\"/home/fabric/work/\"+data_dir_tx1+\"/{flow1}-result.json\".format(flow1=name_tx1), \"r\") as f:\n",
    "        iperf3_data = json.load(f)\n",
    "\n",
    "\n",
    "    throughput_data[name_tx1]  = iperf3_data['end']['sum_received']['bits_per_second']/(1000000*1) # to convert Mbit\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##### Average SRTT for Each Flow ******\n",
    "    \n",
    "    columns = ['timestamp', 'flow ID', 'cwnd', 'srtt']\n",
    "    df_f1= pd.read_csv('/home/fabric/work/'+data_dir_tx0+'/{flow1}-ss.csv'.format(flow1=name_tx0), names=columns)\n",
    "    df_f2= pd.read_csv('/home/fabric/work/'+data_dir_tx1+'/{flow1}-ss.csv'.format(flow1=name_tx1), names=columns)\n",
    "    \n",
    "    # Filter out rows with flow ID = 4, they are for the control flows\n",
    "    df_f1= df_f1[df_f1['flow ID'] != 4]\n",
    "    df_f2= df_f2[df_f2['flow ID'] != 4]\n",
    "    \n",
    "    average_RTT_f1 = df_f1['srtt'].mean()\n",
    "    average_RTT_f2 = df_f2['srtt'].mean()\n",
    "    \n",
    "    srtt_data[name_tx0] = average_RTT_f1\n",
    "    srtt_data[name_tx1] = average_RTT_f2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Placeholder: A dictionary to store throughput data for each experiment\n",
    "# Example format: \"n_bdp_btl_capacity_base_rtt_aqm_ecn_threshold_ecn_fallback_rx_ecn_cc_tx_trial\"\n",
    "# throughput_data = {\n",
    "#     \"prague_0.5_100_10_FIFO_none_0_0_0\": 55.787676690306085,\n",
    "#     \"prague_2.0_100_10_FIFO_none_0_0_0\": 54.248112443427175,\n",
    "#     \"prague_5.0_100_10_FIFO_none_0_0_0\": 38.2013028326071,\n",
    "#     \"prague_10.0_100_10_FIFO_none_0_0_0\": 63.37023092922114,\n",
    "#     \"cubic_0.5_100_10_FIFO_none_0_0_0\": 47.47733567404541,\n",
    "#     \"cubic_2.0_100_10_FIFO_none_0_0_0\": 37.5916393558464,\n",
    "#     \"cubic_5.0_100_10_FIFO_none_0_0_0\": 65.17600721037583,\n",
    "#     \"cubic_10.0_100_10_FIFO_none_0_0_0\": 50.3362326648659,\n",
    "# }\n",
    "\n",
    "\n",
    "# User-specified parameters\n",
    "specified_params = {\n",
    "    'btl_capacity': 100,\n",
    "    'n_bdp': 2,\n",
    "    'base_rtt': 10,\n",
    "    'ecn_threshold': 5,\n",
    "    'ecn_fallback': 0,\n",
    "    'rx0_ecn': 0,\n",
    "    'rx1_ecn': 0\n",
    "    \n",
    "}\n",
    "\n",
    "factor_x = 'aqm' \n",
    "\n",
    "relevant_data_tx0 = {}\n",
    "relevant_data_tx1 = {}\n",
    "\n",
    "relevant_srtt_data_tx0 = {}\n",
    "relevant_srtt_data_tx1 = {}\n",
    "\n",
    "for exp in exp_lists:\n",
    "    #is_relevant = all(item in exp.items() for item in specified_params.items())\n",
    "    \n",
    "    is_relevant = all(\n",
    "        (k == 'ecn_threshold' and (v == exp.get(k) or exp.get(k) is None)) or \n",
    "        (k != 'ecn_threshold' and v == exp.get(k)) for k, v in specified_params.items()\n",
    "    )\n",
    "\n",
    "    if is_relevant:\n",
    "        name_tx0=\"%s_%0.1f_%d_%d_%s_%s_%d_%d_%d\" % (exp['cc_tx0'],exp['n_bdp'], exp['btl_capacity'], exp['base_rtt'], exp['aqm'], str(exp.get('ecn_threshold', 'none')), exp['ecn_fallback'], exp['rx0_ecn'], exp['rx1_ecn'])\n",
    "        name_tx1=\"%s_%0.1f_%d_%d_%s_%s_%d_%d_%d\" % (exp['cc_tx1'],exp['n_bdp'], exp['btl_capacity'], exp['base_rtt'], exp['aqm'], str(exp.get('ecn_threshold', 'none')), exp['ecn_fallback'], exp['rx0_ecn'], exp['rx1_ecn'])\n",
    "\n",
    "        print(exp[factor_x])\n",
    "        if name_tx0 in throughput_data:\n",
    "            xval = exp[factor_x]\n",
    "            if xval not in relevant_data_tx0:\n",
    "                relevant_data_tx0[xval] = []\n",
    "            relevant_data_tx0[xval].append(throughput_data[name_tx0])\n",
    "            \n",
    "        if name_tx1 in throughput_data:\n",
    "            xval = exp[factor_x]\n",
    "            if xval not in relevant_data_tx1:\n",
    "                relevant_data_tx1[xval] = []\n",
    "            relevant_data_tx1[xval].append(throughput_data[name_tx1])\n",
    "            \n",
    "        if name_tx0 in srtt_data:\n",
    "            xval = exp[factor_x]\n",
    "            if xval not in relevant_srtt_data_tx0:\n",
    "                relevant_srtt_data_tx0[xval] = []\n",
    "            relevant_srtt_data_tx0[xval].append(srtt_data[name_tx0])\n",
    "            \n",
    "        if name_tx1 in srtt_data:\n",
    "            xval = exp[factor_x]\n",
    "            if xval not in relevant_srtt_data_tx1:\n",
    "                relevant_srtt_data_tx1[xval] = []\n",
    "            relevant_srtt_data_tx1[xval].append(srtt_data[name_tx1])\n",
    "            \n",
    "        \n",
    "\n",
    "# Average the throughputs over all trials for factor_x\n",
    "for xval, throughputs in relevant_data_tx0.items():\n",
    "    relevant_data_tx0[xval] = np.mean(throughputs)\n",
    "    \n",
    "for xval, throughputs in relevant_data_tx1.items():\n",
    "    relevant_data_tx1[xval] = np.mean(throughputs)\n",
    "    \n",
    "for xval, srtts in relevant_srtt_data_tx0.items():\n",
    "    relevant_srtt_data_tx0[xval] = np.mean(srtts)\n",
    "    \n",
    "for xval, srtts in relevant_srtt_data_tx1.items():\n",
    "    relevant_srtt_data_tx1[xval] = np.mean(srtts)\n",
    "\n",
    "\n",
    "# Sort values\n",
    "xvals = sorted(list(set(list(relevant_data_tx0.keys()) + list(relevant_data_tx1.keys()))))\n",
    "\n",
    "print(xvals)\n",
    "\n",
    "# Get throughputs for sorted values\n",
    "throughputs_tx0 = [relevant_data_tx0.get(xval, 0) for xval in xvals]\n",
    "throughputs_tx1 = [relevant_data_tx1.get(xval, 0) for xval in xvals]\n",
    "\n",
    "rtts_tx0 = [relevant_srtt_data_tx0.get(xval, 0) for xval in xvals]\n",
    "rtts_tx1 = [relevant_srtt_data_tx1.get(xval, 0) for xval in xvals]\n",
    "\n",
    "print(rtts_tx0)\n",
    "print(rtts_tx1)\n",
    "bar_width = 0.35\n",
    "index = np.arange(len(xvals))\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "bar1 = plt.bar(index, throughputs_tx0, bar_width, label='TX0', alpha=0.8, color='b')\n",
    "bar2 = plt.bar(index + bar_width, throughputs_tx1, bar_width, label='TX1', alpha=0.8, color='r')\n",
    "\n",
    "# Annotate bars for TX0\n",
    "for bar in bar1:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, height + 0.5, f\"{height:.2f}\", ha='center', va='bottom')\n",
    "\n",
    "# Annotate bars for TX1\n",
    "for bar in bar2:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, height + 0.5, f\"{height:.2f}\", ha='center', va='bottom')\n",
    "\n",
    "# Label the bars and the x-axis\n",
    "plt.xlabel(factor_x)\n",
    "plt.ylabel('Average Throughput')\n",
    "plt.title('Average Throughput vs queue type for different flows')\n",
    "plt.xticks(index + bar_width/2, xvals)  # Positioning on the x axis\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "bar_width = 0.35\n",
    "index = np.arange(len(xvals))\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "bar1 = plt.bar(index, rtts_tx0, bar_width, label='TX0', alpha=0.8, color='b')\n",
    "bar2 = plt.bar(index + bar_width, rtts_tx1, bar_width, label='TX1', alpha=0.8, color='r')\n",
    "\n",
    "# Annotate bars for TX0\n",
    "for bar in bar1:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, height + 0.5, f\"{height:.2f}\", ha='center', va='bottom')\n",
    "\n",
    "# Annotate bars for TX1\n",
    "for bar in bar2:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, height + 0.5, f\"{height:.2f}\", ha='center', va='bottom')\n",
    "\n",
    "# Label the bars and the x-axis\n",
    "plt.xlabel(factor_x)\n",
    "plt.ylabel('Average SRTT')\n",
    "plt.title('Average SRTT vs queue type for different flows')\n",
    "plt.xticks(index + bar_width/2, xvals)  # Positioning on the x axis\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
