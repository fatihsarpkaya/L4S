::: {.cell .markdown}### Analysis of the Results:::::: {.cell .code}```pythoni=0for exp in exp_lists:    if i==5:        break    name_tx0="%s_%0.1f_%d_%d_%s_%s_%d_%d_%d" % (exp['cc_tx0'],exp['n_bdp'], exp['btl_capacity'], exp['base_rtt'], exp['aqm'], str(exp.get('ecn_threshold', 'none')), exp['ecn_fallback'], exp['rx0_ecn'], exp['rx1_ecn'])    name_tx1="%s_%0.1f_%d_%d_%s_%s_%d_%d_%d" % (exp['cc_tx1'],exp['n_bdp'], exp['btl_capacity'], exp['base_rtt'], exp['aqm'], str(exp.get('ecn_threshold', 'none')), exp['ecn_fallback'], exp['rx0_ecn'], exp['rx1_ecn'])            file_out_tx0_csv = name_tx0+"-ss.csv"    stdout_tx0_csv, stderr_tx0_csv = tx0_node.execute("ls " + file_out_tx0_csv, quiet=True)         file_out_tx1_csv = name_tx1+"-ss.csv"    stdout_tx1csv, stderr_tx1_csv = tx0_node.execute("ls " + file_out_tx1_csv, quiet=True)     if len(stdout_tx0_csv) and len(stdout_tx1_csv):        print("Already have " + name_tx0 + " and "+ name_tx1 + ", skipping")    elif len(stderr_tx0_csv) or len(stderr_tx1_csv):        print("Running to generate csv files " + name_tx0 + " and "+ name_tx1)                    #tx0_node.download_file("/home/fabric/work/{flow}-result.json".format(flow=name_prague),"/home/ubuntu/{f_type}-result.json".format(f_type=name_prague))    #tx1_node.download_file("/home/fabric/work/{flow}-result.json".format(flow=name_cubic),"/home/ubuntu/{f_type}-result.json".format(f_type=name_cubic))    #tx0_node.download_file("/home/fabric/work/{flow}-ss.txt".format(flow=name_prague),"/home/ubuntu/{f_type}-ss.txt".format(f_type=name_prague))    #tx1_node.download_file("/home/fabric/work/{flow}-ss.txt".format(flow=name_cubic),"/home/ubuntu/{f_type}-ss.txt".format(f_type=name_cubic))    ss_tx0_script_processing="""        f_1={types};     rm -f ${{f_1}}-ss.csv;    cat ${{f_1}}-ss.txt | sed -e ":a; /<->$/ {{ N; s/<->\\n//; ba; }}"  | grep "iperf3" | grep -v "SYN-SENT"> ${{f_1}}-ss-processed.txt;     cat ${{f_1}}-ss-processed.txt | awk '{{print $1}}' > ts-${{f_1}}.txt;     cat ${{f_1}}-ss-processed.txt | grep -oP '\bcwnd:.*?(\s|$)' |  awk -F '[:,]' '{{print $2}}' | tr -d ' ' > cwnd-${{f_1}}.txt;     cat ${{f_1}}-ss-processed.txt | grep -oP '\brtt:.*?(\s|$)' |  awk -F '[:,]' '{{print $2}}' | tr -d ' '  | cut -d '/' -f 1   > srtt-${{f_1}}.txt;     cat ${{f_1}}-ss-processed.txt | grep -oP '\bfd=.*?(\s|$)' |  awk -F '[=,]' '{{print $2}}' | tr -d ')' | tr -d ' '   > fd-${{f_1}}.txt;    paste ts-${{f_1}}.txt fd-${{f_1}}.txt cwnd-${{f_1}}.txt srtt-${{f_1}}.txt -d ',' > ${{f_1}}-ss.csv;""".format(types=name_tx0)             tx0_node.execute(ss_tx0_script_processing)    ss_tx1_script_processing="""        f_2={types};    rm -f ${{f_2}}-ss.csv;    cat ${{f_2}}-ss.txt | sed -e ":a; /<->$/ {{ N; s/<->\\n//; ba; }}"  | grep "iperf3" | grep -v "SYN-SENT" > ${{f_2}}-ss-processed.txt;     cat ${{f_2}}-ss-processed.txt | awk '{{print $1}}' > ts-${{f_2}}.txt;     cat ${{f_2}}-ss-processed.txt | grep -oP '\bcwnd:.*?(\s|$)' |  awk -F '[:,]' '{{print $2}}' | tr -d ' ' > cwnd-${{f_2}}.txt;     cat ${{f_2}}-ss-processed.txt | grep -oP '\brtt:.*?(\s|$)' |  awk -F '[:,]' '{{print $2}}' | tr -d ' '  | cut -d '/' -f 1   > srtt-${{f_2}}.txt;     cat ${{f_2}}-ss-processed.txt | grep -oP '\bfd=.*?(\s|$)' |  awk -F '[=,]' '{{print $2}}' | tr -d ')' | tr -d ' '   > fd-${{f_2}}.txt;    paste ts-${{f_2}}.txt fd-${{f_2}}.txt cwnd-${{f_2}}.txt srtt-${{f_2}}.txt -d ',' > ${{f_2}}-ss.csv;""".format(types=name_tx1)    tx1_node.execute(ss_tx1_script_processing)    #tx0_node.download_file("/home/fabric/work/{f_type}-ss.csv".format(f_type=name_prague),"/home/ubuntu/{f_type}-ss.csv".format(f_type=name_prague))    #tx1_node.download_file("/home/fabric/work/{f_type}-ss.csv".format(f_type=name_cubic),"/home/ubuntu/{f_type}-ss.csv".format(f_type=name_cubic))    i=i+1tx0_node.execute('rm -r '+data_dir_tx0)tx0_node.execute('mkdir '+data_dir_tx0)tx0_node.execute('mv *.json '+ data_dir_tx0)tx0_node.execute('mv *.txt '+ data_dir_tx0)tx0_node.execute('mv *.csv '+ data_dir_tx0)tx0_node.execute('tar -czvf '+data_dir_tx0+ '.tgz ' +  data_dir_tx0)tx0_node.download_file(data_dir_tx0+'.tgz ', '/home/ubuntu/' + data_dir_tx0+ '.tgz')tx1_node.execute('rm -r '+data_dir_tx1)tx1_node.execute('mkdir '+data_dir_tx1)tx1_node.execute('mv *.json '+ data_dir_tx1)tx1_node.execute('mv *.txt '+ data_dir_tx1)tx1_node.execute('mv *.csv '+ data_dir_tx1)        tx1_node.execute('tar -czvf '+data_dir_tx1+ '.tgz ' +  data_dir_tx1)tx1_node.download_file(data_dir_tx1+'.tgz ', '/home/ubuntu/' + data_dir_tx1+ '.tgz')```:::::: {.cell .code}```pythonimport jsonimport numpy as npimport matplotlib.pyplot as pltimport pandas as pdimport refrom statistics import mean#get all throughput datathroughput_dict = {}  # Initialize the dictionaryfor exp in exp_lists:    name_tx0="%s_%0.1f_%d_%d_%s_%s_%d_%d_%d" % (exp['cc_tx0'],exp['n_bdp'], exp['btl_capacity'], exp['base_rtt'], exp['aqm'], str(exp.get('ecn_threshold', 'none')), exp['ecn_fallback'], exp['rx0_ecn'], exp['rx1_ecn'])    name_tx1="%s_%0.1f_%d_%d_%s_%s_%d_%d_%d" % (exp['cc_tx1'],exp['n_bdp'], exp['btl_capacity'], exp['base_rtt'], exp['aqm'], str(exp.get('ecn_threshold', 'none')), exp['ecn_fallback'], exp['rx0_ecn'], exp['rx1_ecn'])        # Load the JSON output file into a Python object    ##### Average Throughput for Each Flow ****        with open("{flow1}-result.json".format(flow1=name_tx0), "r") as f:        iperf3_data = json.load(f)    throughput_dict[name_tx0]  = iperf3_data['end']['sum_received']['bits_per_second']/(1000000*1) # to convert Mbit    with open("{flow1}-result.json".format(flow1=name_tx1), "r") as f:        iperf3_data = json.load(f)    throughput_dict[name_tx1]  = iperf3_data['end']['sum_received']['bits_per_second']/(1000000*1) # to convert Mbitimport matplotlib.pyplot as pltimport numpy as np# Placeholder: A dictionary to store throughput data for each experiment# Example format: "n_bdp_btl_capacity_base_rtt_aqm_ecn_threshold_ecn_fallback_rx_ecn_cc_tx_trial"throughput_data = {    "prague_0.5_100_10_FIFO_none_0_0_0": 55.787676690306085,    "prague_2.0_100_10_FIFO_none_0_0_0": 54.248112443427175,    "prague_5.0_100_10_FIFO_none_0_0_0": 38.2013028326071,    "prague_10.0_100_10_FIFO_none_0_0_0": 63.37023092922114,    "cubic_0.5_100_10_FIFO_none_0_0_0": 47.47733567404541,    "cubic_2.0_100_10_FIFO_none_0_0_0": 37.5916393558464,    "cubic_5.0_100_10_FIFO_none_0_0_0": 65.17600721037583,    "cubic_10.0_100_10_FIFO_none_0_0_0": 50.3362326648659,}# User-specified parametersspecified_params = {    'btl_capacity': 100,    'base_rtt': 10,    'aqm': 'FIFO',    'ecn_fallback': 0,    'rx0_ecn': 0,    'rx1_ecn': 0    }relevant_data_tx0 = {}relevant_data_tx1 = {}    for exp in exp_lists:    is_relevant = all(item in exp.items() for item in specified_params.items())    if is_relevant:        name_tx0="%s_%0.1f_%d_%d_%s_%s_%d_%d_%d" % (exp['cc_tx0'],exp['n_bdp'], exp['btl_capacity'], exp['base_rtt'], exp['aqm'], str(exp.get('ecn_threshold', 'none')), exp['ecn_fallback'], exp['rx0_ecn'], exp['rx1_ecn'])        name_tx1="%s_%0.1f_%d_%d_%s_%s_%d_%d_%d" % (exp['cc_tx1'],exp['n_bdp'], exp['btl_capacity'], exp['base_rtt'], exp['aqm'], str(exp.get('ecn_threshold', 'none')), exp['ecn_fallback'], exp['rx0_ecn'], exp['rx1_ecn'])        if name_tx0 in throughput_data:            n_bdp = exp['n_bdp']            if n_bdp not in relevant_data_tx0:                relevant_data_tx0[n_bdp] = []            relevant_data_tx0[n_bdp].append(throughput_data[name_tx0])                    if name_tx1 in throughput_data:            n_bdp = exp['n_bdp']            if n_bdp not in relevant_data_tx1:                relevant_data_tx1[n_bdp] = []            relevant_data_tx1[n_bdp].append(throughput_data[name_tx1])# Average the throughputs over all trials for each n_bdpfor n_bdp, throughputs in relevant_data_tx0.items():    relevant_data_tx0[n_bdp] = np.mean(throughputs)    for n_bdp, throughputs in relevant_data_tx1.items():    relevant_data_tx1[n_bdp] = np.mean(throughputs)# Sort BDP valuesbdps = sorted(list(set(list(relevant_data_tx0.keys()) + list(relevant_data_tx1.keys()))))# Get throughputs for sorted BDP valuesthroughputs_tx0 = [relevant_data_tx0.get(bdp, 0) for bdp in bdps]throughputs_tx1 = [relevant_data_tx1.get(bdp, 0) for bdp in bdps]bar_width = 0.35index = np.arange(len(bdps))plt.figure(figsize=(10,6))bar1 = plt.bar(index, throughputs_tx0, bar_width, label='TX0', alpha=0.8, color='b')bar2 = plt.bar(index + bar_width, throughputs_tx1, bar_width, label='TX1', alpha=0.8, color='r')# Annotate bars for TX0for bar in bar1:    height = bar.get_height()    plt.text(bar.get_x() + bar.get_width()/2, height + 0.5, f"{height:.2f}", ha='center', va='bottom')# Annotate bars for TX1for bar in bar2:    height = bar.get_height()    plt.text(bar.get_x() + bar.get_width()/2, height + 0.5, f"{height:.2f}", ha='center', va='bottom')# Label the bars and the x-axisplt.xlabel('n_bdp')plt.ylabel('Average Throughput')plt.title('Average Throughput vs n_bdp for different flows')plt.xticks(index + bar_width/2, bdps)  # Positioning on the x axisplt.legend()plt.tight_layout()plt.show()                                                                columns = ['timestamp', 'flow ID', 'cwnd', 'srtt']        df_f1= pd.read_csv('{flow1}-ss.csv'.format(flow1=name_prague), names=columns)        df_f2= pd.read_csv('{flow1}-ss.csv'.format(flow1=name_cubic), names=columns)                # Filter out rows with flow ID = 4, they are for the control flows        df_f1= df_f1[df_f1['flow ID'] != 4]            df_f2= df_f2[df_f2['flow ID'] != 4]            ##### Average SRTT for Each Flow ******            average_RTT_f1 = df_f1['srtt'].mean()        average_RTT_f2 = df_f2['srtt'].mean()                with open('{scenario}_summary.txt'.format(scenario=name_prague), 'w') as f:            f.write('Average Throughput of {flow} (Mbit/s) = '.format(flow="Prague") + str(flow1_throughput) +'\n')            f.write('Average Throughput of {flow} (Mbit/s) = '.format(flow="Cubic") + str(flow2_throughput) +'\n')            f.write('Average SRTT of {flow} (ms) = '.format(flow="Prague")+ str(average_RTT_f1) +'\n')            f.write('Average SRTT of {flow} (ms) = '.format(flow="Cubic")+ str(average_RTT_f2) +'\n')                fig, ax = plt.subplots(figsize=(30,18))        df_f1['timestamp'] -= df_f1['timestamp'].min()        df_f2['timestamp'] -= df_f2['timestamp'].min()        plt.plot(df_f1['timestamp'], df_f1['cwnd'], label="Prague")        plt.plot(df_f2['timestamp'], df_f2['cwnd'], label="Cubic")        plt.legend()        ax.legend(loc='upper left', bbox_to_anchor=(1, 1))        plt.ylabel('cwnd')        plt.xlabel('Timestamp (s)')        #plt.title('FIFO, ECN, 2 BDP Bottleneck (15 ms threshold) - prague_ecn_fallback ON vs OFF')        plt.title("Prague vs Cubic - " + name_scenario)        plt.savefig(name_scenario+".pdf", bbox_inches='tight')```:::